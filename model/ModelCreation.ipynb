{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd55700-2c72-4ed8-bb24-5f3e078ec393",
   "metadata": {},
   "source": [
    "# Initial setup\n",
    "setup of the Dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43843532-8076-4603-9faf-0da139ef204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import decode_image\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "import math\n",
    "class MMRFineTune(Dataset):\n",
    "    def istrain(path: Path) -> bool:\n",
    "        return path.parts[-2].contains('train')\n",
    "    def __init__(self, root: Path, transform=None):\n",
    "        if isinstance(root, str):\n",
    "            self.root = Path(root)\n",
    "        elif isinstance(root, Path):\n",
    "            self.root = root\n",
    "        self.transform = transform\n",
    "        self.other_list = list(self.root.glob('other/*.jpg'))\n",
    "        self.train_list = list(self.root.glob('train/*.jpg'))\n",
    "        ds = load_dataset(\"zh-plus/tiny-imagenet\")\n",
    "        self.contrast = ds['train']\n",
    "        self.max_contrast = 0 #max num of contrast samples\n",
    "        self.n_other = len(self.other_list)\n",
    "        self.n_train = len(self.train_list)\n",
    "    def __len__(self):\n",
    "        return self.max_contrast + self.n_other + self.n_train\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.max_contrast:\n",
    "            image = torchvision.transforms.functional.pil_to_tensor(self.contrast[idx]['image'])\n",
    "            is_train = False\n",
    "        elif idx < self.max_contrast + self.n_other:\n",
    "            img_path = self.other_list[idx - self.max_contrast]\n",
    "            image = decode_image(img_path)\n",
    "            is_train = False\n",
    "        else:\n",
    "            img_path = self.train_list[idx - self.max_contrast - self.n_other]\n",
    "            image = decode_image(img_path)\n",
    "            is_train = True\n",
    "        \n",
    "        num_objs = 1\n",
    "        _, h,w = image.shape\n",
    "        boxes = torch.zeros((num_objs, 4), dtype=torch.float)\n",
    "        boxes[0,0] = math.floor(w * 0.1)\n",
    "        boxes[0,1] = math.floor(h * 0.1)\n",
    "        boxes[0,2] = math.floor(w * 0.9)\n",
    "        boxes[0,3] = math.floor(h * 0.9)\n",
    "        \n",
    "        if is_train:\n",
    "            labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        else:\n",
    "            labels = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        target = {}\n",
    "        #print('imgesize: ', type(torchvision.transforms.functional.get_image_size(image)))\n",
    "        #print('shape: ', image.shape)\n",
    "        target[\"boxes\"] = torchvision.tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(h,w))\n",
    "        \n",
    "        #target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = idx\n",
    "        target[\"area\"] = area\n",
    "        \n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "        return image, target\n",
    "class MMRVideos(Dataset):\n",
    "    def read_files(self):\n",
    "        frame_store = []\n",
    "        last_frame = []\n",
    "        for video in (self.root / Path('train')).iterdir():\n",
    "            frames = sorted([str(frame) for frame in video.glob('*.png')])\n",
    "            frame_store += frames\n",
    "            last_frame += [False]*(len(frames)-1) + [True]\n",
    "        self.frame_store = frame_store\n",
    "        self.last_frame = last_frame \n",
    "        self.frame_offset = list(itertools.accumulate(last_frame))\n",
    "        self.max_id = len(self)\n",
    "\n",
    "    def __init__(self, root: Path, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.read_files()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_store) - self.frame_offset[-1]\n",
    "        \n",
    "    def __getitem__(self, lidx):\n",
    "        idx = lidx + self.frame_offset[lidx]\n",
    "        img_path = self.frame_store[idx]\n",
    "        image = decode_image(img_path)\n",
    "                \n",
    "        if not self.last_frame[idx] and idx < self.max_id-1:\n",
    "            nx_frame = decode_image(self.frame_store[idx+1])\n",
    "        else:\n",
    "            nx_frame = None\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            if nx_frame is not None:\n",
    "                nx_frame = self.transform(nx_frame)\n",
    "        return image, nx_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd04721-a8f8-4f59-9989-b8694379d093",
   "metadata": {},
   "source": [
    "### Testrun of the model to test if data works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef92be-2135-4f05-9ed1-c766e619baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import decode_image\n",
    "person_int = decode_image(str(Path(\"./data/train/woodbridge\") / \"8.png\"))\n",
    "#weights = torchvision.models.get_weight('FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1')\n",
    "model = torchvision.models.get_model('mobilenet_v3_large', weights='IMAGENET1K_V2')\n",
    "weights = torchvision.models.get_weight('MobileNet_V3_Large_Weights.IMAGENET1K_V2')\n",
    "transforms = weights.transforms()\n",
    "\n",
    "person_float = transforms(person_int).unsqueeze(0)\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "#model.avgpool = torch.nn.Identity()\n",
    "#model.classifier = torch.nn.Identity() \n",
    "#model.features.register_forward_hook(get_activation(\"feats\"))\n",
    "model = model.eval()\n",
    "outputs = model(person_float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203fbda-82cc-4f11-b9ac-ab84f0084eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "\n",
    "\n",
    "from torchvision.utils import draw_keypoints\n",
    "kpts = outputs[0]['keypoints']\n",
    "scores = outputs[0]['scores']\n",
    "\n",
    "detect_threshold = 0.75\n",
    "idx = torch.where(scores > detect_threshold)\n",
    "keypoints = kpts[idx]\n",
    "res = draw_keypoints(person_int, keypoints, colors=\"blue\", radius=3)\n",
    "show(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79c5650-da9d-459c-b325-6306443ddba9",
   "metadata": {},
   "source": [
    "# Fine tune backbone for this task\n",
    "The model requires a image feature encoder. As a backbone Imagenet is used.\n",
    "To further enhance the extraction features will be extracted via a feature pyramid.\n",
    "This is also intented to be flexible for future modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d04c4-9405-41ee-9422-d931c3dcf479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.mobilenetv3 import _mobilenet_v3_conf\n",
    "def get_model_traindetector(num_classes=2, dropout=0.2):\n",
    "    model = torchvision.models.mobilenet_v3_large(weights=\"IMAGENET1K_V2\")\n",
    "    weights = torchvision.models.get_weight('MobileNet_V3_Large_Weights.IMAGENET1K_V2')\n",
    "    #stop\n",
    "    # get number of input features for the classifier\n",
    "    #in_features = model.classifier.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    inverted_residual_setting, last_channel = _mobilenet_v3_conf(\"mobilenet_v3_large\")\n",
    "    #feat_dim = inverted_residual_setting[-1].out_channels\n",
    "    feat_dim = 960\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(feat_dim, last_channel),\n",
    "            torch.nn.Hardswish(inplace=True),\n",
    "            torch.nn.Dropout(p=dropout, inplace=True),\n",
    "            torch.nn.Linear(last_channel, num_classes),\n",
    "        )#train+other\n",
    "    for m in model.classifier.modules():\n",
    "        if isinstance(m, torch.nn.Conv2d):\n",
    "            torch.nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (torch.nn.BatchNorm2d, torch.nn.GroupNorm)):\n",
    "            torch.nn.init.ones_(m.weight)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(m.weight, 0, 0.01)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    #copy first layer of classif from pretrained\n",
    "    state_dict = weights.get_state_dict()\n",
    "    with torch.no_grad():\n",
    "        model.classifier[0].weight.copy_(state_dict['classifier.0.weight'])\n",
    "        model.classifier[0].bias.copy_(state_dict['classifier.0.bias'])\n",
    "    # now get the number of input features for the mask classifier\n",
    "    def nograds(layer_name):\n",
    "        #no_grads = [f'features.{x}' for x in range(15)]\n",
    "        no_grads = ['classifier.3']\n",
    "        return any(map(lambda x: layer_name.startswith(x), no_grads))\n",
    "    for name, param in model.named_parameters(): \n",
    "     \n",
    "        #if name.startswith('features') and not (name.startswith('features.15.block.2') or name.startswith('features.15.block.3') or name.startswith('features.16')):\n",
    "        if not nograds(name):\n",
    "          param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823cf9c-ad7f-47fb-b815-dcb75fe1e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detector.engine import train_one_epoch\n",
    "import torchvision.transforms.v2 as v2\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model_traindetector()\n",
    "model_raw = get_model_traindetector()\n",
    "\n",
    "\n",
    "transform = v2.Compose([\n",
    " v2.Resize(232),\n",
    " v2.CenterCrop(224),\n",
    " v2.ToDtype(torch.float32, scale=True),\n",
    " v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #taken from imagenet config\n",
    "])\n",
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    #target = torch.LongTensor(target)\n",
    "    return [data, target]\n",
    "\n",
    "dataset = MMRFineTune('finetune', transform=transform)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    #collate_fn=my_collate,\n",
    ")\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "# let's train it just for 2 epochs\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    #evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baac833-ae9d-45cb-a4a8-111a2b21198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.COCO_V1\n",
    "model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=weights)\n",
    "#model(person_float)\n",
    "# get number of input features for the classifier\n",
    "#model.roi_heads.box_predictor.cls_score.in_features\n",
    "#import copy\n",
    "#from customssd import SSDLiteHead\n",
    "#model.head.regression_head\n",
    "return_nodes = { 'head':'reghead'}\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "create_feature_extractor(model, return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43938a5-9f4d-4dd3-8938-bfd11de36042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####TEST ONLY\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "#import utils\n",
    "transform = v2.Compose([\n",
    " v2.Resize(size=(320,320)),\n",
    " #v2.CenterCrop((600, 480)),\n",
    " #v2.CenterCrop((64, 64)),\n",
    " v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    #target = torch.LongTensor(target)\n",
    "    return [data, target]\n",
    "\n",
    "dataset = MMRFineTune('finetune', transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=my_collate,\n",
    ")\n",
    "images, targets = next(iter(data_loader))\n",
    "images = [image for image in images]\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "from importlib import reload\n",
    "reload(detector)\n",
    "from detector.ssdlite import custom_ssdlite320_mobilenet_v3_large\n",
    "weights = torchvision.models.detection.SSDLite320_MobileNet_V3_Large_Weights.COCO_V1\n",
    "model = custom_ssdlite320_mobilenet_v3_large(num_classes=2, trainable_backbone_layers=1, weights=weights)\n",
    "model.eval()\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212af601-c8b4-4e47-9c07-79d970583a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
    "from torchvision.models.detection.backbone_utils import LastLevelMaxPool\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "\n",
    "train_nodes, eval_nodes = get_graph_node_names(model)\n",
    "\n",
    "\n",
    "\n",
    "class FrameEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, outdim=10):\n",
    "        return_nodes = {\n",
    "            'features.14.add': 'l14',\n",
    "            'features.15.add':'l15',\n",
    "            'features.16':'l16',\n",
    "        }\n",
    "        super(FrameEncoder, self).__init__()\n",
    "        m = torchvision.models.get_model('mobilenet_v3_large', weights='IMAGENET1K_V2')\n",
    "        self.body = create_feature_extractor(m, return_nodes=return_nodes)\n",
    "         # Dry run to get number of channels for FPN\n",
    "        inp = torch.randn(2, 3, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            out = self.body(inp)\n",
    "        in_channels_list = [o.shape[1] for o in out.values()]\n",
    "        # Build FPN\n",
    "        self.out_channels = outdim\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list, out_channels=self.out_channels,\n",
    "            extra_blocks=LastLevelMaxPool())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = self.fpn(x)\n",
    "        return x\n",
    "class KeypointExtractor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, image_dim: Tuple):\n",
    "        super(KeypointExtractor, self).__init__()\n",
    "        max_keypoints = 10\n",
    "        n_bins = 2\n",
    "        self.backbone = FrameEncoder(outdim=max_keypoints)\n",
    "        self.vertical_classifier = torch.nn.Linear(10, n_bins*image_h)\n",
    "        self.h_classifier = torch.nn.Linear(10, n_bins*image_w)\n",
    "    def forward(self, x):\n",
    "        x = F.flatten(axis=-1)\n",
    "        h_feat = self.vertical_classifier(x)\n",
    "        w_feat = self.h_classifier(x)\n",
    "        h_conf, h_feat = torch.max(h_feat, dim=1)\n",
    "        w_conf, w_feat = torch.max(w_feat, dim=1)\n",
    "        keypoints = torch.cat(h_feat, w_feat, 0)\n",
    "        return keypoints, torch.cat(h_conf, w_conf, 0)\n",
    "m = FrameEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f8bd1-2a70-405b-873a-7c4df686e8ff",
   "metadata": {},
   "source": [
    "### Trainingarchitecture\n",
    "\n",
    "the attempted method follows XXX et al with a motion difference reconstruction loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
